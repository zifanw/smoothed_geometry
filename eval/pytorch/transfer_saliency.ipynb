{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transfer_saliency.ipynb","provenance":[{"file_id":"1evahbQ5Yzo3eOPIyKHM-gT_RfYGWW8yV","timestamp":1587858609464}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Dh24rT-pi-N0","colab_type":"code","colab":{}},"source":["pip install -r requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGBwZk8E_Tal","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import math\n","import glob\n","import random\n","import argparse\n","import numpy as np\n","from PIL import Image\n","import scipy.stats as stats\n","import scipy.spatial as spatial\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","\n","import dill\n","from robustness.datasets import ImageNet\n","from robustness.model_utils import make_and_restore_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSEypuWUwOKs","colab_type":"code","colab":{}},"source":["def convert_relu_to_softplus(model):\n","  for child_name, child in model.named_children():\n","      if isinstance(child, nn.ReLU):\n","          setattr(model, child_name, nn.Softplus())\n","      else:\n","          convert_relu_to_softplus(child)\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9dybpSW1L-6u","colab_type":"code","colab":{}},"source":["data_path = 'data/'\n","image_path = 'data/images'\n","cls_label = open(os.path.join(data_path,'labels.txt'))\n","cls_label = cls_label.readlines()\n","print(len(cls_label))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LohTP1vNwegu","colab_type":"text"},"source":["# Attack and Evaluate"]},{"cell_type":"code","metadata":{"id":"dYdGtFxCwflK","colab_type":"code","colab":{}},"source":["def main(method=\"topK\", top_K=1000, max_iter=50, max_epsilon=2.0, baseline_image=None, transfer_target=\"UG\"):\n","\n","  eps = max_epsilon / 255.0\n","\n","  count = 0\n","\n","  center_dislocation_sum = 0\n","  correlation_sum = 0\n","  intersection_sum = 0\n","  cosine_distance_sum = 0\n","\n","  # generate a target map\n","  if method == \"manipulate\":\n","    line = cls_label[-1]\n","    line = line.strip('\\n').split(' ')\n","    name = os.path.join(image_path,line[0])\n","    label = int(line[1])\n","\n","    image = Image.open(name).convert('RGB')\n","    image = image.resize((224, 224), Image.ANTIALIAS)\n","    image = np.array(image) / 255.0\n","\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    # get the saliency map using softplus model\n","    logit,_ = vgg_softplus(inputs)\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    target_map = torch.autograd.grad(torch.sum(logit[0]*one_hot_output), inputs, create_graph=True)[0] * inputs\n","    target_map = target_map.squeeze().detach()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_target_map = torch.sum(torch.abs(target_map),0)\n","    normalized_target_map = 224*224*normalized_target_map/torch.sum(normalized_target_map)\n","\n","  # start the loop\n","  tic = time.time()\n","  for line in cls_label[:200]:\n","    count += 1\n","\n","    line = line.strip('\\n').split(' ')\n","    name = os.path.join(image_path,line[0])\n","    label = int(line[1])\n","\n","    image = Image.open(name).convert('RGB')\n","    image = image.resize((224, 224), Image.ANTIALIAS)\n","    image = np.array(image) / 255.0\n","\n","    '''\n","    original result on softplus model\n","    '''\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    # get the saliency map using softplus model\n","    logit,_ = vgg_softplus(inputs)\n","    original_logit,_ = vgg_softplus(inputs)\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    original_saliency = torch.autograd.grad(torch.sum(logit[0]*one_hot_output), inputs, create_graph=True)[0] * inputs\n","    original_saliency = original_saliency.squeeze().detach()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","    normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","    normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","    # get the mass center of original saliency map\n","    y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","    y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","    mass_center = [int(torch.sum(normalized_original_saliency*x_mesh)/(224*224)), int(torch.sum(normalized_original_saliency*y_mesh)/(224*224))]\n","\n","    #print(\"original mass center of softplus model:\", mass_center)\n","\n","    '''\n","    generate attacked input\n","    '''\n","    adv_image = inputs.clone()\n","    best_adv_image = inputs.clone()\n","    for i in range(max_iter):\n","      adv_image = adv_image.clone().detach().requires_grad_(True)\n","\n","      # get the saliency map of current adversarial image\n","      logit,_ = vgg_softplus(adv_image)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      saliency = torch.autograd.grad(torch.sum(logit[0]*one_hot_output), adv_image, create_graph=True)[0] * adv_image\n","      saliency = saliency.squeeze()\n","\n","      # normalize the saliency map\n","      saliency = torch.sum(torch.abs(saliency),0)\n","      saliency = 224*224*saliency/torch.sum(saliency)\n","      saliency_flatten = saliency.flatten()\n","\n","      # find the gradient direction\n","      if method == \"topK\":\n","        mask = torch.zeros(224*224).cuda()\n","        mask[torch.argsort(saliency.view(-1))[-top_K:]] = 1\n","        topK_loss = -torch.sum(saliency_flatten*mask)\n","        topK_direction = torch.autograd.grad(topK_loss, adv_image)[0]\n","        grad_sign = topK_direction.sign()\n","      elif method == \"mass_center\":\n","        mass_center_perturbed = [torch.sum(saliency*x_mesh)/(224*224), torch.sum(saliency*y_mesh)/(224*224)]\n","        mass_center_loss = ((mass_center[0]-mass_center_perturbed[0])**2 + (mass_center[1]-mass_center_perturbed[1])**2)\n","        mass_center_direction = torch.autograd.grad(mass_center_loss, adv_image)[0]\n","        grad_sign = mass_center_direction.sign()\n","      elif method == \"target\":\n","        if target_map is None:\n","          raise ValueError(\"target map is None!\")\n","        else:\n","          target_loss = torch.sum(saliency*target_map)\n","          grad_sign = torch.autograd.grad(target_loss, adv_image)[0].sign()\n","      elif method == \"manipulate\":\n","        # if target_map is None:\n","        #   raise ValueError(\"target map is None!\")\n","        # else:\n","\n","        loss_expl = F.mse_loss(saliency, normalized_target_map)\n","        loss_output = F.mse_loss(original_logit, logit)\n","        total_loss = 1e11*loss_expl + 1e6*loss_output\n","        grad_sign = - torch.autograd.grad(total_loss, adv_image)[0].sign()\n","      else:\n","        grad_sign = torch.randn(adv_image.shape).cuda().sign()\n","\n","      # apply perturbation\n","      adv_image = inputs + torch.clamp(adv_image+0.005*grad_sign-inputs, -eps, eps)\n","      adv_image = torch.clamp(adv_image, 0, 1)\n","\n","      # evaluate each adv_image on given metric, save the best adv_image\n","      if vgg(inputs)[0].max(1)[-1] == vgg(adv_image)[0].max(1)[-1]:\n","          best_adv_image = adv_image.clone()\n","      \n","    '''\n","    Evaluate on metrics\n","    '''\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    if transfer_target == \"IG\":\n","      '''\n","      S transfer to IG\n","      '''\n","      # find intergated inputs\n","      num_steps = 50\n","      baseline_image = torch.zeros(3,224,224).cuda() if baseline_image is None else baseline_image\n","      counterfactuals = [(float(i+1)/num_steps) * (inputs-baseline_image) + baseline_image for i in range(num_steps)]\n","      counterfactuals = torch.cat(counterfactuals)\n","      counterfactuals = counterfactuals.requires_grad_(True)\n","\n","      # get the IG map using relu model\n","      logit,_ = vgg(counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0].detach()\n","      original_saliency = torch.mean(original_saliency, dim=0)\n","      original_saliency = original_saliency * (counterfactuals[-1]-baseline_image).squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","      normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","      normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","      # get the mass center of original map\n","      y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","      y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","      mass_center = [torch.sum(normalized_original_saliency*x_mesh)/(224*224), torch.sum(normalized_original_saliency*y_mesh)/(224*224)]\n","\n","      # get perturbed input\n","      adv_image = best_adv_image.clone()\n","      adv_image = adv_image.requires_grad_(True)\n","\n","      # find intergated perturbed inputs\n","      perturb_counterfactuals = [(float(i+1)/num_steps) * (adv_image-baseline_image) + baseline_image for i in range(num_steps)]\n","      perturb_counterfactuals = torch.cat(perturb_counterfactuals).requires_grad_(True)\n","\n","      # get the perturbed saliency map using relu model\n","      logit,_ = vgg(perturb_counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      perturb_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), perturb_counterfactuals)[0].detach()\n","      perturb_saliency = torch.mean(perturb_saliency, dim=0)\n","      perturb_saliency = perturb_saliency * (perturb_counterfactuals[-1]-baseline_image).squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_perturb_saliency = torch.sum(torch.abs(perturb_saliency),0)\n","      normalized_perturb_saliency = 224*224*normalized_perturb_saliency/torch.sum(normalized_perturb_saliency)\n","      normalized_perturb_saliency_flatten = normalized_perturb_saliency.flatten()\n","\n","      # get the mass center of perturbed map\n","      mass_center_perturbed = [torch.sum(normalized_perturb_saliency*x_mesh)/(224*224), torch.sum(normalized_perturb_saliency*y_mesh)/(224*224)]\n","    \n","    elif transfer_target == \"SG\":\n","      '''\n","      S transfer to SG\n","      '''\n","      # find intergated inputs\n","      num_steps = 50\n","      sigma = 0.2 * (torch.max(inputs) - torch.min(inputs)).item()\n","      counterfactuals = [inputs+inputs.data.new(inputs.size()).normal_(0, sigma**2).cuda() for i in range(num_steps)]\n","      counterfactuals = torch.cat(counterfactuals)\n","      counterfactuals = counterfactuals.requires_grad_(True)\n","\n","      # get the SG map using relu model\n","      logit,_ = vgg(counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0]\n","      original_saliency = torch.mean(original_saliency, dim=0)\n","      original_saliency = original_saliency * inputs.squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","      normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","      normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","      # get the mass center of original map\n","      y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","      y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","      mass_center = [torch.sum(normalized_original_saliency*x_mesh)/(224*224), torch.sum(normalized_original_saliency*y_mesh)/(224*224)]\n","\n","      # get perturbed input\n","      adv_image = best_adv_image.clone()\n","\n","      sigma = 0.2 / (torch.max(adv_image) - torch.min(adv_image)).item()\n","      perturb_counterfactuals = [adv_image+adv_image.data.new(adv_image.size()).normal_(0, sigma**2).cuda() for i in range(num_steps)]\n","      perturb_counterfactuals = torch.cat(perturb_counterfactuals).requires_grad_(True)\n","\n","      # get the perturbed saliency map using relu model\n","      logit,_ = vgg(perturb_counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      perturb_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), perturb_counterfactuals)[0]\n","      perturb_saliency = torch.mean(perturb_saliency, dim=0)\n","      perturb_saliency = perturb_saliency * adv_image.squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_perturb_saliency = torch.sum(torch.abs(perturb_saliency),0)\n","      normalized_perturb_saliency = 224*224*normalized_perturb_saliency/torch.sum(normalized_perturb_saliency)\n","      normalized_perturb_saliency_flatten = normalized_perturb_saliency.flatten()\n","\n","      # get the mass center of perturbed map\n","      mass_center_perturbed = [torch.sum(normalized_perturb_saliency*x_mesh)/(224*224), torch.sum(normalized_perturb_saliency*y_mesh)/(224*224)]\n","    \n","    elif transfer_target == \"UG\":\n","      '''\n","      S transfer to UG\n","      '''\n","      # find intergated inputs\n","      num_steps = 50\n","      sigma = 0.2 / (torch.max(inputs) - torch.min(inputs)).item()\n","      counterfactuals = [inputs+inputs.data.new(inputs.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","      counterfactuals = torch.cat(counterfactuals)\n","      counterfactuals = counterfactuals.requires_grad_(True)\n","\n","      # get the UG map using relu model\n","      logit,_ = vgg(counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0]\n","      original_saliency = torch.mean(original_saliency, dim=0)\n","      original_saliency = original_saliency * inputs.squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","      normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","      normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","      # get the mass center of original map\n","      y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","      y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","      mass_center = [torch.sum(normalized_original_saliency*x_mesh)/(224*224), torch.sum(normalized_original_saliency*y_mesh)/(224*224)]\n","\n","      # get perturbed input\n","      adv_image = best_adv_image.clone()\n","\n","      sigma = 0.2 / (torch.max(adv_image) - torch.min(adv_image)).item()\n","      perturb_counterfactuals = [adv_image+adv_image.data.new(adv_image.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","      perturb_counterfactuals = torch.cat(perturb_counterfactuals).requires_grad_(True)\n","\n","      # get the perturbed map using relu model\n","      logit,_ = vgg(perturb_counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      perturb_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), perturb_counterfactuals)[0]\n","      perturb_saliency = torch.mean(perturb_saliency, dim=0)\n","      perturb_saliency = perturb_saliency * adv_image.squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten map into 1D \n","      normalized_perturb_saliency = torch.sum(torch.abs(perturb_saliency),0)\n","      normalized_perturb_saliency = 224*224*normalized_perturb_saliency/torch.sum(normalized_perturb_saliency)\n","      normalized_perturb_saliency_flatten = normalized_perturb_saliency.flatten()\n","\n","      # get the mass center of perturbed map\n","      mass_center_perturbed = [torch.sum(normalized_perturb_saliency*x_mesh)/(224*224), torch.sum(normalized_perturb_saliency*y_mesh)/(224*224)]\n","    \n","    else:\n","      raise ValueError(\"Transfer target is not supported!\")\n","\n","    # evaluation\n","    center_dislocation = torch.sqrt(((mass_center[0]-mass_center_perturbed[0])**2 + (mass_center[1]-mass_center_perturbed[1])**2)).cpu().detach().numpy()\n","    correlation = stats.spearmanr(normalized_original_saliency_flatten.cpu().detach().numpy(), normalized_perturb_saliency_flatten.cpu().detach().numpy())\n","\n","    top_val, top_idx = torch.topk(normalized_original_saliency_flatten, top_K)\n","    pert_val, pert_idx = torch.topk(normalized_perturb_saliency_flatten, top_K)\n","    intersection = float(len(np.intersect1d(top_idx.cpu().detach().numpy(), pert_idx.cpu().detach().numpy())))/top_K\n","    cosine_distance = float(spatial.distance.cosine(normalized_original_saliency_flatten.cpu().detach().numpy(), normalized_perturb_saliency_flatten.cpu().detach().numpy()))\n","\n","    center_dislocation_sum += center_dislocation\n","    correlation_sum += correlation[0]\n","    intersection_sum += intersection\n","    cosine_distance_sum += cosine_distance\n","\n","    torch.cuda.empty_cache()\n","\n","  print(\"#######################\")\n","  print(method, max_epsilon, transfer_target)\n","  print(\"It spend {} to process {}/{}.\".format(time.time()-tic, count, len(cls_label)))\n","  print('average center dislocation:', center_dislocation_sum / count)\n","  print('average correlation:', correlation_sum / count)\n","  print('average intersection:', intersection_sum / count)\n","  print('average cosine distance', cosine_distance_sum / count)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KpJ9bT7kcyf","colab_type":"text"},"source":["# Evaluate the transferabity"]},{"cell_type":"code","metadata":{"id":"03Pxrzmfas7l","colab_type":"code","colab":{}},"source":["# the path of targeted model\n","model_path = None\n","\n","# load model\n","ds = ImageNet('/path/to/imagenet')\n","if model_path is None:\n","  model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=None, pytorch_pretrained=True)\n","  softplus_model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=None, pytorch_pretrained=True)\n","else:\n","  model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","             resume_path=model_path, pytorch_pretrained=False)\n","  softplus_model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=model_path, pytorch_pretrained=False)\n","\n","vgg = model.eval().cuda()\n","vgg_softplus = convert_relu_to_softplus(softplus_model).eval().cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j38VA0kmSqeA","colab_type":"code","colab":{}},"source":["for eps in [2.0,4.0,8.0,16.0]:\n","  main(method=\"topK\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"IG\")\n","  main(method=\"topK\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"SG\")\n","  main(method=\"topK\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"UG\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OParByDPSpqO","colab_type":"code","colab":{}},"source":["for eps in [2.0,4.0,8.0,16.0]:\n","  main(method=\"manipulate\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"IG\")\n","  main(method=\"manipulate\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"SG\")\n","  main(method=\"manipulate\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"UG\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bP6vMvagadm-","colab_type":"code","colab":{}},"source":["for eps in [2.0,4.0,8.0,16.0]:\n","  main(method=\"mass_center\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"IG\")\n","  main(method=\"mass_center\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"SG\")\n","  main(method=\"mass_center\", top_K=1000, max_iter=50, max_epsilon=eps, baseline_image=None, transfer_target=\"UG\")"],"execution_count":0,"outputs":[]}]}
