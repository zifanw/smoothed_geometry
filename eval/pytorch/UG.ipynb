{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UG.ipynb","provenance":[{"file_id":"1-1z7F3ib4ribAmIHzrT-sq1ToWkzoRix","timestamp":1587852407627}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"CArlJu7h4F8d","colab_type":"code","colab":{}},"source":["pip install -r requirements.txt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEQQiKsliHcR","colab_type":"text"},"source":["# Import dependence"]},{"cell_type":"code","metadata":{"id":"v-QhD_XG4HLo","colab_type":"code","colab":{}},"source":["import os\n","import time\n","import math\n","import glob\n","import random\n","import argparse\n","import numpy as np\n","from PIL import Image\n","import scipy.stats as stats\n","import scipy.spatial as spatial\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","\n","import dill\n","from robustness.datasets import ImageNet\n","from robustness.model_utils import make_and_restore_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8FnTmtQNhUo","colab_type":"code","colab":{}},"source":["def convert_relu_to_softplus(model):\n","  for child_name, child in model.named_children():\n","      if isinstance(child, nn.ReLU):\n","          setattr(model, child_name, nn.Softplus())\n","      else:\n","          convert_relu_to_softplus(child)\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lb0crL-64Zhi","colab_type":"text"},"source":["# Hyper params"]},{"cell_type":"code","metadata":{"id":"ifRIHtfEh42H","colab_type":"code","colab":{}},"source":["# the path of targeted model\n","model_path = None\n","\n","# define top K\n","top_K = 1000\n","\n","# max iter\n","max_iter = 50\n","\n","# params for UG\n","num_steps = 50"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-icFoyCt4Mou","colab_type":"text"},"source":["# Load model and data"]},{"cell_type":"code","metadata":{"id":"YGBwZk8E_Tal","colab_type":"code","colab":{}},"source":["ds = ImageNet('/path/to/imagenet')\n","\n","if model_path is None:\n","  model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=None, pytorch_pretrained=True)\n","  softplus_model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=None, pytorch_pretrained=True)\n","else:\n","  model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","             resume_path=model_path, pytorch_pretrained=False)\n","  softplus_model, _, = make_and_restore_model(arch='resnet50', dataset=ds,\n","              resume_path=model_path, pytorch_pretrained=False)\n","\n","vgg = model.eval().cuda()\n","vgg_softplus = convert_relu_to_softplus(softplus_model).eval().cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKpBhZkyKqar","colab_type":"code","colab":{}},"source":["data_path = 'data/'\n","image_path = 'data/images'\n","cls_label = open(os.path.join(data_path,'labels.txt'))\n","cls_label = cls_label.readlines()\n","print(len(cls_label))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p4PCLYj2iLE3","colab_type":"text"},"source":["# Attack and Evaluate"]},{"cell_type":"code","metadata":{"id":"ARXBbioUNu-B","colab_type":"code","colab":{}},"source":[" def main(method = \"topK\", max_epsilon = 2.0):\n","  count = 0\n","\n","  eps = max_epsilon / 255.0\n","\n","  center_dislocation_sum = 0\n","  correlation_sum = 0\n","  intersection_sum = 0\n","  cosine_distance_sum = 0\n","\n","  # generate a target map\n","  if method == \"manipulate\":\n","    line = cls_label[-1]\n","    line = line.strip('\\n').split(' ')\n","    name = os.path.join(image_path,line[0])\n","    label = int(line[1])\n","\n","    image = Image.open(name).convert('RGB')\n","    image = image.resize((224, 224), Image.ANTIALIAS)\n","    image = np.array(image) / 255.0\n","\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    # get the saliency map using softplus model\n","    logit,_ = vgg_softplus(inputs)\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    target_map = torch.autograd.grad(torch.sum(logit[0]*one_hot_output), inputs, create_graph=True)[0] * inputs\n","    target_map = target_map.squeeze().detach()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_target_map = torch.sum(torch.abs(target_map),0)\n","    normalized_target_map = 224*224*normalized_target_map/torch.sum(normalized_target_map)\n","    normalized_target_map = normalized_target_map.detach()\n","\n","  # start the loop\n","  tic = time.time()\n","  for line in cls_label:\n","    count += 1\n","\n","    line = line.strip('\\n').split(' ')\n","    name = os.path.join(image_path,line[0])\n","    label = int(line[1])\n","\n","    image = Image.open(name).convert('RGB')\n","    image = image.resize((224, 224), Image.ANTIALIAS)\n","    image = np.array(image) / 255.0\n","\n","    '''\n","    original result on softplus model\n","    '''\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    # find intergated inputs\n","    sigma = 0.2 * (torch.max(inputs) - torch.min(inputs)).item()\n","    counterfactuals = [inputs+inputs.data.new(inputs.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","    counterfactuals = torch.cat(counterfactuals)\n","    counterfactuals = counterfactuals.requires_grad_(True)\n","\n","    # get the UG map using softplus model\n","    logit,_ = vgg_softplus(counterfactuals)\n","    original_logit = logit.clone()\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0].detach()\n","    original_saliency = torch.mean(original_saliency, dim=0)\n","    original_saliency = original_saliency * inputs.squeeze()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","    normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","    normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","    # get the mass center of original saliency map\n","    y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","    y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","    mass_center = [int(torch.sum(normalized_original_saliency*x_mesh)/(224*224)), int(torch.sum(normalized_original_saliency*y_mesh)/(224*224))]\n","\n","\n","    '''\n","    attack\n","    '''\n","    adv_image = inputs.detach().clone()\n","    best_adv_image = adv_image.clone()\n","    max_mass_center_loss = 0\n","    for i in range(max_iter):\n","      \n","      adv_image = adv_image.clone().detach().requires_grad_(True)\n","\n","      # find intergated inputs\n","      sigma = 0.2 / (torch.max(adv_image) - torch.min(adv_image)).item()\n","      counterfactuals = [adv_image+adv_image.data.new(adv_image.size()).uniform_(-sigma, sigma) for i in range(num_steps)]\n","      counterfactuals = torch.cat(counterfactuals)\n","\n","      # get the SG map using softplus model\n","      logit,_ = vgg_softplus(counterfactuals)\n","      one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","      one_hot_output[0][label] = 1\n","      if method == 'random':\n","        saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals, create_graph=False)[0]\n","      else:\n","        saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals, create_graph=True)[0]\n","      saliency = torch.mean(saliency, dim=0)\n","      saliency = saliency * adv_image.squeeze()\n","\n","      # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","      saliency = torch.sum(torch.abs(saliency),0)\n","      saliency = 224*224*saliency/torch.sum(saliency)\n","      saliency_flatten = saliency.flatten()\n","\n","      # find the gradient direction\n","      if method == \"topK\":\n","        mask = torch.zeros(224*224).cuda()\n","        mask[torch.argsort(saliency.view(-1))[-top_K:]] = 1\n","        topK_loss = -torch.sum(saliency_flatten*mask)\n","        topK_direction = torch.autograd.grad(topK_loss, adv_image)[0]\n","        grad_sign = topK_direction.sign()\n","      elif method == \"mass_center\":\n","        mass_center_perturbed = [torch.sum(saliency*x_mesh)/(224*224), torch.sum(saliency*y_mesh)/(224*224)]\n","        mass_center_loss = ((mass_center[0]-mass_center_perturbed[0])**2 + (mass_center[1]-mass_center_perturbed[1])**2)\n","        mass_center_direction = torch.autograd.grad(mass_center_loss, adv_image)[0]\n","        grad_sign = mass_center_direction.sign()\n","      elif method == \"manipulate\":\n","        # if target_map is None:\n","        #   raise ValueError(\"target map is None!\")\n","        # else:\n","        loss_expl = F.mse_loss(saliency, normalized_target_map)\n","        loss_output = F.mse_loss(original_logit, logit)\n","        total_loss = 1e11*loss_expl + 1e6*loss_output\n","        grad_sign = - torch.autograd.grad(total_loss, adv_image)[0].sign()\n","      elif method == \"target\":\n","        if target_map is None:\n","          raise ValueError(\"target map is None!\")\n","        else:\n","          target_loss = torch.sum(saliency*target_map)\n","          grad_sign = torch.autograd.grad(target_loss, adv_image)[0].sign()\n","      else:\n","        grad_sign = torch.randn(adv_image.shape).cuda().sign()\n","\n","      # apply perturbation\n","      adv_image = inputs + torch.clamp(adv_image+0.005*grad_sign-inputs, -eps, eps)\n","      adv_image = torch.clamp(adv_image, 0, 1)\n","\n","      # evaluate each adv_image on given metric, save the best adv_image\n","      if vgg(inputs)[0].max(1)[-1] == vgg(adv_image)[0].max(1)[-1]:\n","          best_adv_image = adv_image.detach().clone()\n","\n","    '''\n","    Evaluate on metrics\n","    '''\n","    # transfer Image into tensor, with shape NxCxHxW\n","    inputs = torch.from_numpy(image.transpose((2,0,1)))\n","    inputs = inputs.unsqueeze(0).float().requires_grad_(True).cuda()\n","\n","    # find intergated inputs\n","    sigma = 0.2 / (torch.max(inputs) - torch.min(inputs)).item()\n","    counterfactuals = [inputs+inputs.data.new(inputs.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","    counterfactuals = torch.cat(counterfactuals)\n","    counterfactuals = counterfactuals.requires_grad_(True)\n","\n","    # get the UG map using softplus model\n","    logit,_ = vgg(counterfactuals)\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0]\n","    original_saliency = torch.mean(original_saliency, dim=0)\n","    original_saliency = original_saliency * inputs.squeeze()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_original_saliency = torch.sum(torch.abs(original_saliency),0)\n","    normalized_original_saliency = 224*224*normalized_original_saliency/torch.sum(normalized_original_saliency)\n","    normalized_original_saliency_flatten = normalized_original_saliency.flatten()\n","\n","    # get the mass center of original saliency map\n","    y_mesh, x_mesh = np.meshgrid(np.arange(224),np.arange(224))\n","    y_mesh, x_mesh = torch.Tensor(y_mesh).cuda(), torch.Tensor(x_mesh).cuda()\n","    mass_center = [torch.sum(normalized_original_saliency*x_mesh)/(224*224), torch.sum(normalized_original_saliency*y_mesh)/(224*224)]\n","\n","    # get perturbed input\n","    adv_image = best_adv_image.clone()\n","\n","    sigma = 0.2 / (torch.max(adv_image) - torch.min(adv_image)).item()\n","    perturb_counterfactuals = [adv_image+adv_image.data.new(adv_image.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","    perturb_counterfactuals = torch.cat(perturb_counterfactuals).requires_grad_(True)\n","\n","    # get the perturbed saliency map using relu model\n","    logit,_ = vgg(perturb_counterfactuals)\n","    one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","    one_hot_output[0][label] = 1\n","    perturb_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), perturb_counterfactuals)[0]\n","    perturb_saliency = torch.mean(perturb_saliency, dim=0)\n","    perturb_saliency = perturb_saliency * adv_image.squeeze()\n","\n","    # combine color channel; normalized into (0,1) and scale by image size; flatten saliency map into 1D \n","    normalized_perturb_saliency = torch.sum(torch.abs(perturb_saliency),0)\n","    normalized_perturb_saliency = 224*224*normalized_perturb_saliency/torch.sum(normalized_perturb_saliency)\n","    normalized_perturb_saliency_flatten = normalized_perturb_saliency.flatten()\n","\n","    # get the mass center of perturbed saliency map\n","    mass_center_perturbed = [torch.sum(normalized_perturb_saliency*x_mesh)/(224*224), torch.sum(normalized_perturb_saliency*y_mesh)/(224*224)]\n","\n","    center_dislocation = torch.sqrt(((mass_center[0]-mass_center_perturbed[0])**2 + (mass_center[1]-mass_center_perturbed[1])**2)).cpu().detach().numpy()\n","    correlation = stats.spearmanr(normalized_original_saliency_flatten.cpu().detach().numpy(), normalized_perturb_saliency_flatten.cpu().detach().numpy())\n","\n","    top_val, top_idx = torch.topk(normalized_original_saliency_flatten, top_K)\n","    pert_val, pert_idx = torch.topk(normalized_perturb_saliency_flatten, top_K)\n","    intersection = float(len(np.intersect1d(top_idx.cpu().detach().numpy(), pert_idx.cpu().detach().numpy())))/top_K\n","    cosine_distance = float(spatial.distance.cosine(normalized_original_saliency_flatten.cpu().detach().numpy(), normalized_perturb_saliency_flatten.cpu().detach().numpy()))\n","\n","    center_dislocation_sum += center_dislocation\n","    correlation_sum += correlation[0]\n","    intersection_sum += intersection\n","    cosine_distance_sum += cosine_distance\n","\n","    torch.cuda.empty_cache()\n","\n","  print(\"#######################\")\n","  print('method:'+method+' '+'eps:'+str(max_epsilon))\n","  print(\"It spend {} to process {}/{}.\".format(time.time()-tic, count, len(cls_label)))\n","  print('average center dislocation:', center_dislocation_sum / count)\n","  print('average correlation:', correlation_sum / count)\n","  print('average intersection:', intersection_sum / count)\n","  print('average cosine distance', cosine_distance_sum / count)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j3-90oY-Y3SO","colab_type":"code","colab":{}},"source":["main(method = \"topK\", max_epsilon = 2.0)\n","main(method = \"topK\", max_epsilon = 4.0)\n","main(method = \"topK\", max_epsilon = 8.0)\n","main(method = \"topK\", max_epsilon = 16.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbZJDGiNY3rk","colab_type":"code","colab":{}},"source":["main(method = \"manipulate\", max_epsilon = 2.0)\n","main(method = \"manipulate\", max_epsilon = 4.0)\n","main(method = \"manipulate\", max_epsilon = 8.0)\n","main(method = \"manipulate\", max_epsilon = 16.0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dS3iIf-QY32E","colab_type":"code","colab":{}},"source":["main(method = \"mass_center\", max_epsilon = 2.0)\n","main(method = \"mass_center\", max_epsilon = 4.0)\n","main(method = \"mass_center\", max_epsilon = 8.0)\n","main(method = \"mass_center\", max_epsilon = 16.0)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LFHw2mHV5XBP","colab_type":"text"},"source":["# Visualization"]},{"cell_type":"code","metadata":{"id":"GMw7xoPu5WfX","colab_type":"code","colab":{}},"source":["# for visual comparison\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","\n","mpl.rcParams[\"figure.figsize\"] = 8,8\n","plt.rc(\"text\",usetex=False)\n","plt.rc(\"font\",family = \"sans-serif\",size = 12)\n","\n","'''\n","visualization\n","'''\n","# find intergated inputs\n","sigma = 0.2 / (torch.max(inputs) - torch.min(inputs)).item()\n","counterfactuals = [inputs+inputs.data.new(inputs.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","counterfactuals = torch.cat(counterfactuals)\n","counterfactuals = counterfactuals.requires_grad_(True)\n","\n","# get the UG map using relu model\n","logit,_ = vgg(counterfactuals)\n","one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","one_hot_output[0][label] = 1\n","original_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), counterfactuals)[0]\n","original_saliency = torch.mean(original_saliency, dim=0)\n","original_saliency = original_saliency * inputs.squeeze()\n","\n","saliency = np.sum(np.abs(original_saliency.detach().cpu().numpy()),0)\n","original_saliency = 224*224*saliency/np.sum(saliency)\n","plt.subplot(2,2,1)\n","plt.title(\"Original Image\")\n","plt.imshow(inputs.squeeze().permute(1,2,0).detach().cpu().numpy())\n","plt.subplot(2,2,2)\n","plt.title(\"Original Image Saliency Map\")\n","plt.imshow(original_saliency,cmap=\"hot\")\n","\n","# find intergated adv image\n","sigma = 0.2 / (torch.max(adv_image) - torch.min(adv_image)).item()\n","perturb_counterfactuals = [adv_image+adv_image.data.new(adv_image.size()).uniform_(-sigma, sigma).cuda() for i in range(num_steps)]\n","perturb_counterfactuals = torch.cat(perturb_counterfactuals).requires_grad_(True)\n","\n","# get the IG map using relu model\n","logit,_ = vgg(perturb_counterfactuals)\n","one_hot_output = torch.FloatTensor(1, logit.size()[-1]).zero_().cuda()\n","one_hot_output[0][label] = 1\n","perturb_saliency = torch.autograd.grad(torch.sum(logit*one_hot_output), perturb_counterfactuals)[0]\n","perturb_saliency = torch.mean(perturb_saliency, dim=0)\n","perturb_saliency = perturb_saliency * adv_image.squeeze()\n","\n","saliency = np.sum(np.abs(perturb_saliency.detach().cpu().numpy()),0)\n","perturbed_saliency = 224*224*saliency/np.sum(saliency)\n","plt.subplot(2,2,3)\n","plt.title(\"Perturbed Image\")\n","plt.imshow(adv_image.squeeze().detach().permute(1,2,0).cpu().numpy())\n","plt.subplot(2,2,4)\n","plt.title(\"Perturbed Image Saliency Map\")\n","plt.imshow(perturbed_saliency,cmap=\"hot\")"],"execution_count":0,"outputs":[]}]}
